# Pythia-1B Configuration for 4-Operand Addition Task
# This config fine-tunes EleutherAI's Pythia-1B model on arithmetic tasks

# ===== Model Selection ===== #
use_llama = True  # Set to True to use pre-trained model
llama_model_name = "EleutherAI/pythia-1b"

# ===== Evaluation Settings ===== #
eval_interval = 1000  # More frequent eval for smaller model
eval_iters = 1
always_save_checkpoint = True

# ===== WandB Logging ===== #
wandb_log = True
wandb_project = 'addition_pythia'
wandb_run_name = '4_operands_pythia_1b_plain'

# ===== Model Architecture (Pythia-specific) ===== #
block_size = 128  # Larger for BPE efficiency
dropout = 0.1  # Light dropout for fine-tuning

# ===== Training Configuration ===== #
# Pythia-1B is smaller than LLaMA-8B, so we can use larger batch size
batch_size = 128  # Can use larger batch than LLaMA (was 64 for LLaMA-8B)
gradient_accumulation_steps = 1

# ===== Data Format ===== #
data_format = 'plain'
operator = '+'
dataset = 'bal'

# ===== Initialization ===== #
# Always use 'scratch' when using pre-trained models - loads pre-trained weights then fine-tunes
init_from = 'scratch'
iter_num = 0
# To resume fine-tuning, change to: init_from = 'resume'
ckpt_path_name = 'ckpt.pt'

# ===== Task Settings ===== #
num_digit = 3
max_new_tokens = 8  # BPE needs fewer tokens than character-level
drop_leading_digit = False
zero_pad = False

# ===== Learning Rate (Lower for Fine-tuning) ===== #
learning_rate = 1e-4  # Lower than custom GPT
max_iters = 30000  # Pythia-1B may converge faster than LLaMA-8B
lr_decay_iters = 30000
beta1 = 0.9
beta2 = 0.999  # Higher beta2 for fine-tuning
warmup_iters = 500
weight_decay = 1e-1
grad_clip = 1.0

# ===== Device ===== #
device = 'cuda:0'
dtype = 'bfloat16'  # Use bfloat16 for efficiency
compile = True  # Use PyTorch 2.0 compile

# ===== Paths ===== #
out_dir = 'results/4_operands_0_to_999_uniform/pythia_out'
data_dir = 'data/4_operands_0_to_999_uniform/'

# ===== Data Files ===== #
train_data_name = 'train.txt'
val_data_name = 'val.txt'
test_file_name = 'test.txt'
main_test_name = "test"

# ===== Evaluation Settings ===== #
eval_addition = True
eval_addition_train = True
train_data_test_name = "train_eval.txt"

# Mode for evaluation: "compute_gold" or "read_gold_as_str"
mode = "read_gold_as_str"

# ===== Statistical Measurements ===== #
# Disable MI measurement (not yet adapted for BPE tokenization)
mi_measurement = False

# ===== Additional Settings ===== #
eval_only = False  # Set to True to only run evaluation
save_final = True
use_flash = True  # Will try Flash Attention, fallback to standard if unavailable

# ===== Notes ===== #
# 1. Pythia-1B is fully open source - NO HuggingFace authentication needed!
#
# 2. Memory requirements: ~4-6GB GPU memory (much less than LLaMA-8B)
#
# 3. Expected convergence: 15k-30k iterations (faster than LLaMA-8B)
#
# 4. Training speed: ~50-200 it/s depending on GPU (faster than LLaMA-8B)
#
# 5. Model info:
#    - Parameters: 1B (vs 8B for LLaMA, ~3M for custom GPT)
#    - Architecture: GPT-NeoX
#    - Trained on The Pile dataset
#    - Vocabulary: ~50k tokens
